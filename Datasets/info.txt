1) Ryerson Audio-Visual Database of Emotional Speech and Song

The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) is a multimodal database of emotional speech and song.
The RAVDESS contains 7356 files (total size: 24.8 GB). Each file was rated 10 times on emotional validity, intensity, and genuineness.
Ratings were provided by 247 individuals who were characteristic of untrained adult research participants from North America.
A further set of 72 participants provided test-retest data.
High levels of emotional validity, inter-rater reliability, and test-retest intra-rater reliability were reported.

Key features of RAVDESS:

Modalities: Both audio and visual data.
Emotions: The dataset includes multiple emotions such as neutral, calm, happy, sad, angry, fearful, surprise, and disgust.
Intensities: Both normal and strong intensity variations are present for each emotion.
Actors: The dataset includes 24 professional actors (12 female, 12 male), who vocalize two lexically-matched statements in a neutral North American accent.
Speech and Song: The dataset differentiates between speech and song, offering a unique opportunity for studies comparing these two modalities.
File Naming Convention:
The filenames consist of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4):

Modality (01 = full-AV, 02 = video-only, 03 = audio-only).
Vocal channel (01 = speech, 02 = song).
Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).
Emotional intensity (01 = normal, 02 = strong). Note: There is no strong intensity for the 'neutral' emotion.
Statement (01 = "Kids are talking by the door", 02 = "Dogs are sitting by the door").
Repetition (01 = 1st repetition, 02 = 2nd repetition).
Actor (01 to 24. Odd numbered actors are male, even-numbered actors are female).
For example, the filename 02-01-06-01-02-01-12.mp4 is:

Video-only modality
Speech
Fearful emotion
Normal intensity
Statement "Dogs are sitting by the door"
1st repetition
12th actor (female)
This dataset is freely available for research purposes and has been used in various studies related to emotion recognition, audio-visual analysis, and other areas.






2) IEMOCAP: Contains dyadic sessions with audio-visual data including multiple emotions.

Yes, the IEMOCAP (Interactive Emotional Dyadic Motion Capture) database is another widely recognized and utilized resource in the field of emotion recognition.
Unlike EMO-DB, which focuses solely on audio data, IEMOCAP provides both audio and visual data, capturing a richer set of emotional cues.
Here are some key details about the IEMOCAP database:

1. **Content**: The IEMOCAP database contains approximately 12 hours of audio-visual data from dyadic interactions.
It was collected from 10 actors (5 male, 5 female) over five sessions.
In each session, a male and a female actor performed scripts or had spontaneous conversations in a variety of emotional states.

2. **Emotions**: The database covers a wide range of emotions, including but not limited to:
   - Happiness
   - Sadness
   - Anger
   - Frustration
   - Excitement
   - Fear
   - Surprise
   - Neutral
   
3. **Modalities**: The database includes multiple modalities:
   - Audio recordings
   - Video recordings
   - 3D motion capture data of facial expressions
   - Text transcriptions
   
4. **Annotations**: The data in IEMOCAP is annotated at both the categorical level (e.g., happiness, sadness) and the dimensional level (e.g., valence, arousal, dominance).
Some annotations are based on self-assessment by the actors, while others are based on evaluations by multiple human annotators.

5. **Applications**: IEMOCAP is versatile and has been used for a range of research purposes, including:
   - Audio-visual emotion recognition
   - Speech emotion recognition
   - Facial expression analysis
   - Multimodal emotion analysis
   - Emotion synthesis for avatars and virtual agents
   
6. **License**: IEMOCAP is available for research purposes, but access is subject to certain terms and conditions.

The IEMOCAP database was created by the Speech Analysis and Interpretation Lab (SAIL) at the University of Southern California (USC).
If you're interested in obtaining the dataset, you would typically need to contact SAIL at USC and request access, ensuring that you adhere to their licensing conditions.

Again, it's always crucial to respect the terms of use and licensing agreements when using datasets.
If you have more questions or need further details about IEMOCAP, feel free to ask!






3) EMO-DB: A German database containing samples of emotional speech.

Yes, the EMO-DB (Berlin Database of Emotional Speech) is a widely used database in the field of speech emotion recognition.
It contains samples of emotional speech in German. Here are some key details about the EMO-DB:

1. **Languages and Speakers**: The database contains samples from 10 German-speaking actors (5 male, 5 female). These actors simulated seven different emotions in a studio setting.
  
2. **Emotions**: The seven emotions represented in the database are:
   - Anger
   - Boredom
   - Disgust
   - Fear
   - Happiness
   - Sadness
   - Neutral
   
3. **Content**: The actors read a series of German sentences, which are phonetically rich.
Each of these sentences is recorded in each of the seven emotional states.

4. **Format**: The recordings are available in WAV format.

5. **Applications**: The EMO-DB has been used for various purposes, including:
   - Training and testing emotion recognition systems
   - Psychological and phonetic studies on emotional speech
   - Developing and testing speech synthesis systems that can convey emotions

6. **Annotations**: Each sample in the database is labeled with the intended emotion.
Some samples also have additional annotations, such as the perceived emotion or the intensity of the emotion.

7. **License**: The database is freely available for non-commercial research purposes.

Researchers and developers have used EMO-DB to train machine learning and deep learning models for emotion recognition from speech, and it remains a benchmark dataset in the field.
If you have specific questions about the EMO-DB or its applications, feel free to ask!
